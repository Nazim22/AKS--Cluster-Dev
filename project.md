ğŸš€ POC: Secure and Automated AKS Cloud Platform
Document Version: 1.2

Status: Proposed

Owner: DevOps Engineering

Date: October 5, 2025

Executive Summary
This document outlines a Proof of Concept (POC) to validate a secure, automated, and production-ready architectural pattern for deploying applications on the Azure Kubernetes Service (AKS). The primary objective is to demonstrate the feasibility of managing the entire infrastructure lifecycle via a GitOps workflow, leveraging Infrastructure as Code (IaC) principles. The successful completion of this POC will establish a standardized blueprint for migrating future workloads to a modern, scalable, and secure cloud-native platform, ensuring seamless and secure integration with existing corporate network and identity services.

ğŸ¯ Business Objective & Problem Statement
Objective
To increase development velocity, enhance security posture, and improve operational efficiency by creating a standardized, automated platform for cloud-native applications.

Problem Statement
The current process for deploying cloud infrastructure can be manual and inconsistent, leading to slower delivery cycles, potential security vulnerabilities, and configuration drift. A lack of a standardized, automated platform creates operational overhead and hinders our ability to scale effectively. This POC addresses the need for a secure, repeatable, and auditable process for infrastructure provisioning and application deployment.

ğŸ—ºï¸ Scope of Work
In-Scope (Deliverables)	Out-of-Scope (Items)
âœ… Automated provisioning of all Azure networking components via Terraform.	âŒ Deployment of complex, multi-tier stateful applications.
âœ… Deployment of a secure administrative access method (Azure Bastion).	âŒ Performance benchmarking, load testing, or formal DR testing.
âœ… Deployment and configuration of a private self-hosted GitHub Actions runner.	âŒ Implementation of advanced observability (monitoring, logging, tracing).
âœ… Automated deployment of a private AKS cluster using the self-hosted runner.	âŒ Detailed cost analysis and financial optimization reporting.
âœ… Configuration and validation of the Hybrid DNS resolution flow.	
âœ… Deployment of a sample stateless application to verify the end-to-end workflow.	

Export to Sheets
ğŸ›£ï¸ Project Phases and Next Steps
This project will be executed in distinct phases, moving from a foundational manual setup to a fully automated, scalable platform. This approach ensures a controlled rollout and sets clear expectations.

Phase 1: Foundational Bootstrap (Manual Setup)

Objective: To solve the initial "chicken-and-egg" problem by creating the absolute minimum resources required to enable automation.

Tasks:

Manually create the Service Principal (App Registration) in Azure AD to act as the identity for our CI/CD pipeline.

Manually create the Azure Storage Account that will securely store the Terraform state file.

Outcome: A secure foundation for all subsequent automated work. This phase is performed once and is not part of the recurring workflow.

Phase 2: Core Infrastructure Automation (This POC)

Objective: To build and validate the core, automated infrastructure platform as defined in this document's scope.

Tasks:

Develop reusable Terraform modules for networking, AKS, Bastion, and the CI/CD runner.

Create a GitHub Actions pipeline that uses the bootstrapped identity to deploy the infrastructure.

Deploy a sample application to verify end-to-end functionality.

Outcome: A working, automated, and secure AKS platform that meets all success criteria.

Phase 3: Cluster Onboarding & Hardening (Next Steps)

Objective: To prepare the validated platform for its first real workloads by adding essential services and governance.

Potential Tasks:

Automate the deployment of an NGINX Ingress Controller for traffic management.

Integrate with Azure Monitor for basic logging and metrics.

Implement Azure Policy to enforce security and compliance standards (e.g., prevent public load balancers).

Create a standardized onboarding guide for the first pilot application team.

Phase 4: Full GitOps & Self-Service (Future Vision)

Objective: To evolve the platform into a true self-service model for application deployments.

Potential Tasks:

Implement a GitOps controller (like ArgoCD or Flux) for continuous application deployment.

Develop standardized patterns for stateful applications using Azure Files/Disk.

Publish a service catalog of reusable Terraform modules for development teams.

ğŸ—ï¸ Technical Architecture Deep Dive
This section details the specific technical configurations and decisions for the core components of the architecture.

VNet and Subnet Address Space:
Â  Â  * Virtual Network (VNet): 10.50.0.0/16 - Provides 65,536 total IP addresses, offering ample room for future expansion.
Â  Â  * Subnets:
Â  Â  Â  Â  * AKSSubnet: 10.50.0.0/23 - Allocates 512 IPs. This supports the Azure CNI requirement for a unique IP per pod and can comfortably host a scaled-out cluster (e.g., up to 15 nodes with the default 30 pods per node).
Â  Â  Â  Â  * RunnerSubnet: 10.50.2.0/27 - Allocates 32 IPs, sufficient for a small pool of CI/CD runner VMs.
Â  Â  Â  Â  * AzureBastionSubnet: 10.50.3.0/27 - Allocates 32 IPs. Microsoft recommends at least a /27 for this dedicated subnet.

AKS Network Profile & Policy:
Â  Â  * Network Profile: We will use Azure CNI. This high-performance network plugin assigns a full VNet IP address to each pod, enabling direct connectivity and compatibility with advanced network features.
Â  Â  * Network Policy: We will enable Azure Network Policy (or alternatively, Calico). This acts as a firewall within our cluster, allowing us to define rules that control which pods can communicate with each other, significantly enhancing intra-cluster security.

Storage Configuration:
Â  Â  * Terraform State File: The Terraform state file will be stored securely in an Azure Blob Storage container, a separate and dedicated resource for this critical function.
Â  Â  * AKS Pod Storage: For this POC's stateless application, we will use the default ephemeral storage on the nodes. Future stateful applications will use Azure Disk (for single-pod, read/write volumes) or Azure Files (for shared storage).

Identity, Access & Permissions:
Â  Â  * CI/CD Pipeline Identity: The GitHub Actions workflow will use an App Registration with a Federated Credential (OIDC) for a modern, secretless authentication approach.
Â  Â  * AKS Cluster Identity: The AKS cluster itself will be assigned a System-Assigned Managed Identity to interact with other Azure resources on its behalf (e.g., creating load balancers or attaching disks).

DNS Implementation Strategy:
Â  Â  1.Â  Azure Private DNS Zone: We will create a private zone for our domain.
Â  Â  2.Â  VNet Link: This zone will be linked to our VNet, making it authoritative for all resources within it.
Â  Â  3.Â  ExternalDNS Controller: We will deploy the ExternalDNS controller to our AKS cluster to automatically create A records in our private zone from Kubernetes Ingress resources.
Â  Â  4.Â  Conditional Forwarding: The on-premises network team will configure the corporate DNS server to forward all queries for hero.org to Azure's internal DNS resolver (168.63.129.16).
Â 

ğŸ›ï¸ Terraform Code Structure
The Terraform project will be organized using a modular approach to promote reusability, maintainability, and consistency. The directory structure is designed to separate the reusable "building blocks" (modules) from the environment-specific "implementations" (live).

Markdown

terraform-aks-platform/
â”œâ”€â”€ modules/                # Contains reusable, generic infrastructure components
â”‚   â”œâ”€â”€ networking/         # Module for VNet, subnets, and NSGs
â”‚   â”‚   â”œâ”€â”€ main.tf
â”‚   â”‚   â”œâ”€â”€ variables.tf
â”‚   â”‚   â””â”€â”€ outputs.tf
â”‚   â”‚
â”‚   â”œâ”€â”€ aks/                # Module for the AKS cluster and node pools
â”‚   â”‚   â”œâ”€â”€ main.tf
â”‚   â”‚   â”œâ”€â”€ variables.tf
â”‚   â”‚   â””â”€â”€ outputs.tf
â”‚   â”‚
â”‚   â”œâ”€â”€ bastion/            # Module for the secure Bastion host
â”‚   â”‚   â”œâ”€â”€ main.tf
â”‚   â”‚   â””â”€â”€ variables.tf
â”‚   â”‚
â”‚   â””â”€â”€ runner-vm/          # Module for the self-hosted GitHub Actions runner VM
â”‚       â”œâ”€â”€ main.tf
â”‚       â””â”€â”€ variables.tf
â”‚
â””â”€â”€ live/                   # Contains environment-specific configurations
    â””â”€â”€ prod/               # Configuration for the 'production' environment
        â”œâ”€â”€ main.tf         # Calls the modules to compose the environment
        â”œâ”€â”€ terraform.tfvars  # Defines variable values specific to this environment
        â””â”€â”€ backend.tf      # Configures the remote state backend for Terraform
Architecture
Sizing and VM Selection:

Component	Recommended Size	Rationale
AKS System Nodepool	2 x Standard_D2s_v5	System pods are lightweight. Two small, cost-effective nodes provide high availability for critical cluster services.
AKS User Nodepool	2 x Standard_D4s_v5	A solid general-purpose starting point (4 vCPUs, 16 GiB RAM). We will enable the cluster autoscaler to scale this nodepool up to 10 nodes based on demand.
Self-Hosted Runner VM	1 x Standard_D2s_v5	CI/CD jobs are bursty. This size (2 vCPUs, 8 GiB RAM) provides a good balance of CPU and fast Premium SSD storage for running Terraform and build jobs efficiently.
Azure Bastion Host	Standard SKU	The Standard SKU is required for features like scaling and is recommended for production environments. Sizing is managed by Azure.

Export to Sheets
Linux Profile (Node Configuration):
Â  Â  We will use the default Azure-tuned Ubuntu image for our AKS nodes. We will configure SSH key access to the nodes during creation, which, combined with Azure Bastion, will allow for secure "break-glass" debugging if ever required.

ğŸ’¼ Strategic Importance & Business Value
This architecture is a strategic enabler for the business, providing:

Enhanced Security Posture: A "zero-trust" network with private endpoints and fine-grained network policies drastically reduces the attack surface.

Increased Development Velocity: The GitOps workflow turns the deployment process into a simple git push, removing manual bottlenecks and accelerating time-to-market.

Operational Excellence: Infrastructure as Code eliminates configuration drift and makes our platform repeatable, auditable, and easy to recover.

Scalability & Cost-Effectiveness: The use of the cluster autoscaler ensures that we only pay for the compute resources we need, automatically scaling to meet user demand.

âœ… Success Criteria
The Proof of Concept will be deemed successful upon the unconditional achievement of the following criteria:

Automation: All Azure infrastructure is successfully provisioned and modified exclusively through the GitHub Actions pipeline triggered by a git push.

Security: The deployed AKS cluster's API server is confirmed to be private and not accessible from the public internet.

Integration: A DNS query for a deployed application's hostname from the corporate network resolves correctly to the private IP of the Ingress Controller.

Functionality: The sample application is successfully deployed and is fully accessible from a client on the corporate network.

ğŸ‘¥ Required Resources & Stakeholders
Azure Platform:
Â  Â  * An active Azure Subscription with Contributor-level permissions.
Â  Â  * Sufficient vCPU quota for the planned VM and AKS node pools.

Personnel & Stakeholders:
Â  Â  * DevOps Team: Project lead and primary executors of the POC.
Â  Â  * Network Engineering Team: Required stakeholder for the configuration of the on-premises DNS conditional forwarder.
Â  Â  * Security & Compliance Team: Stakeholder for architectural review and approval.
